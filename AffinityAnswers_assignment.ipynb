{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement 1\n",
    "Imagine there is a file full of Twitter tweets by various users and you are provided a set of words that indicates racial slurs. Write a program that can indicate the degree of profanity for each sentence in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "(1)We are provided a set of words that indicates racial slurs and their respective contribution to the profanity ranging between 0 to 1.\n",
    "    (2)Assuming that the dictionary has unique words and does not contain any stopwords, the try and except block takes care of stopwords removal by assigning zero weight. \n",
    "     (3)We consider words like racism in it's root word by doing lemmatization.\n",
    "     (4)For validation my notebook takes a txt file as input. \n",
    "    (5)The degree of profanity is assumed to vary between 0 to 1, the negative scores have been ignored considering the tweets represent the negative sentiment of racial slurs. The higher the degree of profanity the more negatively opined the statement is.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the txt file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the txt file\n",
    "with open('C:\\\\Users\\\\muska\\\\Downloads\\\\new.txt') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a recent news article by washinton times, mentioned that blacks and more racist than whites.', 'the discrimination based on race has been an age old issue of the us.', 'superiority complex has been a major cause of the discrimination.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization of sentence\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "sentence=sent_tokenize(lines[0])\n",
    "sentence=[statement.lower() for statement in sentence]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the profanity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree of profanity for a particular sentence is based on the weighted sum of individual profanity of a word rationalised by the length of the statement, in order to counter the effect of lenghty statements. This index ensures that the lenghtier statements don't yield more score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that indicate the degree of profanity for each sentence in the file\n",
    "def getProfanity(line):\n",
    "    words=word_tokenize(line)\n",
    "    lemmatize =[wn.lemmatize(w) for w in words]\n",
    "    score=0\n",
    "    for i in range(len(lemmatize)):\n",
    "        try:\n",
    "            word_score=dict1[lemmatize[i]]\n",
    "            score+=word_score\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    profanity = score*100/float(len(words)-1)\n",
    "    return profanity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of the snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a set of words that indicates racial slurs\n",
    "dict1={'race':0.8,'black':0.5,'white':0.2,'dicrimination':0.9,'superiority':0.6, 'complex':0.6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.375\n",
      "5.714285714285714\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "#use defined function to find profanity for each sentence\n",
    "for j in range(len(sentence)):\n",
    "    print(getProfanity(sentence[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement 2\n",
    "Which is an interesting data set you discovered recently? Why is it your favourite?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer-\n",
    "I recently came across the dataset containing tweets of various users on Covid-19 Vaccination. Scraped relevant data from Twitter including tweets, likes, re-tweets and replies along with itâ€™s timestamp using Tweepy which is a convenient and easy way to access twitter API with python. It's a long dataset containing 22k tweets of various users across the world. I used this dataset for making a Machine Learning model for sentiment analysis on Covid-19 Vaccine. The best thing about this dataset is that it gives the actual views of people towards vaccination like their hesistancy towards it. Because it is based on actual sentiment of people so, this ML model can also be used by government and other organisation for making policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
